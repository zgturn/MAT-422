{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN9cWebVgfJaH2yUwSVN6yu",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zgturn/MAT-422/blob/main/Section_1_2_Zachary_Turner.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# **Section 1.2: Elements of Linear Algebra**\n",
        "\n",
        "We only consider the case of the linear space $V = \\mathbb{R}^n$.\n",
        "\n",
        "### **1.2.1: Linear Spaces**\n",
        "**Definition** (Linear subspace): A linear subspace of $V$ is a subset $U \\subseteq V$ such that for all $\\textbf{u}_1, \\textbf{u}_2 \\in U$ and $\\alpha \\in \\mathbb{R}$, we have that\n",
        "$$\\textbf{u}_1 + \\textbf{u}_2 \\in U$$ and\n",
        "$$\\alpha\\textbf{u}_1 \\in U.$$\n",
        "\n",
        "**Definition** (Span) Let $\\textbf{w}_1, \\dots, \\textbf{w}_m \\in V$. The span of $\\{\\textbf{w}_1, \\dots, \\textbf{w}_m\\}$ is the set of all linear combinations of the $\\textbf{w}_j$'s, denoted $\\text{span}(\\textbf{w}_1, \\dots, \\textbf{w}_m)$\n",
        "\n",
        "The span of these vectors form a linear subspace.\n",
        "\n",
        "**Definition** (Linear independence): A list of vectors $\\textbf{u}_1, \\dots, \\textbf{u}_m$ is linearly independent if and only if\n",
        "$$\\sum_{j = 1}^m \\alpha_j\\textbf{u}_j = \\textbf{0} \\implies \\alpha_j = 0$$\n",
        "for all $j$.\n",
        "\n",
        "This concept of linear independence and span lead to an important concept: the basis of a linear space.\n",
        "\n",
        "**Definition** (Basis) Let $U$ be a linear subspace of $V$. We say that a list of vectors $\\textbf{u}_1, \\dots, \\textbf{u}_m \\in U$ forms a basis of $U$ if\n",
        "\n",
        "(1) $ \\text{span}(\\textbf{u}_1, \\dots, \\textbf{u}_m) = U$ and\n",
        "\n",
        "(2) $ \\textbf{u}_1, \\dots, \\textbf{u}_m$ are linearly independent.\n",
        "\n",
        "Every basis of $U$ has the same number of elements. We call this number the dimension of $U$, denoted $\\text{dim}(U)$.\n",
        "\n",
        "\n",
        "\n",
        "### **1.2.2: Orthogonality**\n",
        "\n",
        "#### **_1.2.2.1: Orthogonal Bases_**\n",
        "**Definition** (Norm and inner product): $\\langle\\textbf{u}, \\textbf{v} \\rangle = \\textbf{u} \\cdot \\textbf{v} = \\sum_i u_iv_i$ and $||u|| = \\sqrt{\\sum_i u_i^2}$.\n",
        "\n",
        "**Definition** (Orthonormality): A list of vectors $\\textbf{u}_1, \\dots, \\textbf{u}_m$ is orthonormal if $\\langle \\textbf{u}_i, \\textbf{u}_k \\rangle = 0$ and $||\\textbf{u}_i|| = 1$ for all $i$ and $j \\neq i$\n",
        "\n",
        "**Lemma** (Properties of orthonormal vectors): Let $\\textbf{u}_1, \\dots, \\textbf{u}_m$ be a list of orthonormal vectors. Then\n",
        "\n",
        "(1) $||\\sum_j \\alpha_j\\textbf{u}_j||^2 = \\sum_j \\alpha_j^2$ and\n",
        "\n",
        "(2) $\\textbf{u}_1, \\dots, \\textbf{u}_m$ are linearly independent\n",
        "\n",
        "Usually, it is not obvious how to find the coordinates of a vector $\\textbf{w}$ in a subspace $U$ given a basis. However, if one has an orthonormal basis, it becomes easy.\n",
        "\n",
        "**Theorem** (Orthonormal basis expansion): Let $\\textbf{q}_1, \\dots, \\textbf{q}_m$ be an orthonormal basis of a linear space $U$, and let $\\textbf{u} \\in U$. Then\n",
        "$$\\textbf{u} = \\sum_{j = 1}^m \\langle \\textbf{u}, \\textbf{q}_j \\rangle \\textbf{q}_j.$$\n",
        "\n",
        "#### **_1.2.2.2: Best Approximation Theorem_**\n",
        "\n",
        "A lot of optimization problems can be boiled down to a simple linear algebra problem. Suppose we have a linear subspace $U \\subseteq V$ and a vector $\\textbf{v} \\notin U$. Despite this, we still want to find the vector $\\textbf{v}^* \\in U$ that is the _closest_ in distance to $\\textbf{v}$. This defines the optimization problem\n",
        "\n",
        "$$\\text{min}_{\\textbf{v}^* \\in U} ||\\textbf{v}^* - \\textbf{v}||.$$\n",
        "\n",
        "**Definition** (Orthogonal projection): Let $U \\subseteq V$ be a linear subspace with orthonormal basis $\\textbf{q}_1, \\dots, \\textbf{q}_m$. The orthogonal projection of $\\textbf{v} \\in V$ on $U$ is defined as\n",
        "$$P_U\\textbf{v} = \\sum_{j = 1}^m \\langle \\textbf{v}, \\textbf{q}_j \\rangle \\textbf{q}_j.$$\n",
        "\n",
        "With the orthogonal projection in hand, we can solve the above approximation problem: the vector in $\\textbf{v}^* \\in U$ closest in distance to $\\textbf{v}$ is $P_U\\textbf{v}$. We summarize the result with a theorem.\n",
        "\n",
        "**Theorem** (Best approximation theorem): Let $U \\subset V$ be a linear subspace with orthonormal basis $\\textbf{q}_1, \\dots, \\textbf{q}_m$ and let $\\textbf{v} \\in V$. Then for any $\\textbf{u} \\in U$, we have that\n",
        "$$||\\textbf{v} - P_U\\textbf{v}|| \\leq ||\\textbf{v} - \\textbf{u}||,$$\n",
        "with equality if $\\textbf{u} = P_u\\textbf{v}$.\n",
        "\n",
        "**Lemma** (Orthogonal decomposition): Let $U \\subseteq V$ be a liear subspace with orthonormal basis $\\textbf{q}_1, \\dots, \\textbf{q}_m$ and let $\\textbf{v} \\in V$. Then for any $\\textbf{u} \\in U$, we have that $\\langle \\textbf{v} - P_U\\textbf{v}, \\textbf{u} \\rangle = 0$. Moreover, $\\textbf{v}$ can be decomposed as $(\\textbf{v} - P_U\\textbf{v}) + P_U\\textbf{v}$, with the two terms being orthogonal.\n",
        "\n",
        "We can create a projection matrix with the following method. Let $\\textbf{q}_1, \\dots, \\textbf{q}_m$ be an orthonormal basis, and set $Q = (\\textbf{q}_1 \\cdots \\textbf{q}_m)$. We note that $Q^T\\textbf{v} = (\\langle \\textbf{v}, \\textbf{q}_1 \\rangle \\cdots \\langle \\textbf{v}, \\textbf{q}_m \\rangle)^T$ lists the coefficients in the exansion of $P_U\\textbf{v}$ over the basis $\\textbf{q}_1, \\dots, \\textbf{q}_m$. Hence, we have that $P = QQ^T$ provides the projection of any vector $\\textbf{v} \\in V$ onto $U$.\n",
        "\n",
        "### **1.2.3: Gram-Schmidt Process**\n",
        "\n",
        "The Gram-Schmidt process is an algorithm that, given any basis, can be used to find another orthonormal basis. Let $\\textbf{u}_1, \\dots, \\textbf{u}_m$ be linearly independent vectors. Suppose we want to find an orthonormal basis to represent $\\text{span}(\\textbf{u}_1, \\dots, \\textbf{u}_m) = U$. The process can be explained as follows: we start with the vector $\\textbf{u_1}$, and for each consective vector, take away the orthogonal projection of the previous iterations.\n",
        "\n",
        "To generate an orthonormal basis, set $\\textbf{v}_1 = \\textbf{u}_1$ and $\\textbf{q}_1 = \\frac{\\textbf{v}_1}{||\\textbf{v}_1||}$. Then we set $\\textbf{v}_2 = \\textbf{u}_2 - \\langle \\textbf{u}_1, \\textbf{q}_1 \\rangle\\textbf{q}_1$ and $\\textbf{q}_2 = \\frac{\\textbf{v}_2}{||\\textbf{v}_2||}$. In general, for $i \\in \\{1, \\dots, m\\}$, we have that $\\textbf{v}_i = \\textbf{u}_i - \\sum_{j=1}^{i-1}\\langle \\textbf{u}_j, \\textbf{q}_j \\rangle\\textbf{q}_j$. The list of vectors $\\textbf{q}_1, \\dots, \\textbf{q}_m$ is orthonormal, and $\\text{span}(\\textbf{u}_1, \\dots, \\textbf{u}_m) = \\text{span}(\\textbf{q}_1, \\dots, \\textbf{q}_m)$."
      ],
      "metadata": {
        "id": "rkY8bW053Rex"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Given an matrix A with columns as the basis of a space, it returns a matrix with\n",
        "# columns as the orthonormal basis\n",
        "def gram_schmidt(A):\n",
        "  (n, m) = A.shape\n",
        "  A_orthogonal = A\n",
        "\n",
        "  for i in range(m):\n",
        "    u = A[:, i]\n",
        "    proj = np.zeros([n])\n",
        "\n",
        "    if i >= 1:\n",
        "      for j in range(i):\n",
        "        proj = proj - np.dot(A[:, i], A_orthogonal[:, j])*A_orthogonal[:, j]\n",
        "\n",
        "    v = u + proj\n",
        "    q = v / np.linalg.norm(v)\n",
        "\n",
        "    A_orthogonal[:, i] = q\n",
        "\n",
        "  return A_orthogonal\n",
        "\n",
        "A = np.array([[4.0, -1.0, 0.0], [2.0, 3.0, 5.0], [-1.0, 7.0, 2.0]])\n",
        "A_orthogonal = gram_schmidt(A)\n",
        "\n",
        "print(A_orthogonal)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I0wi35EDkZ8n",
        "outputId": "f202fc16-2ca3-4756-9141-8b560e5d1e61"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[ 0.87287156 -0.00626298 -0.48790984]\n",
            " [ 0.43643578  0.45719752  0.77491563]\n",
            " [-0.21821789  0.88934313 -0.4018081 ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **1.2.4: Eigenvalues and Eigenvectors**\n",
        "\n",
        "#### **_1.2.4.1: Eigenvalues, Eigenvectors, and Diagnalization_**\n",
        "\n",
        "**Definition** (Eigenvalues and eigenvectors): Let $A \\in \\mathbb{R}^{n \\times n}$. Then $\\lambda \\in \\mathbb{R}$ is an _eigenvalue_ of $A$ if there exists $\\textbf{x} \\in \\mathbb{R}^n \\setminus{\\{\\textbf{0}\\}}$ such that\n",
        "$$A\\textbf{x} = \\lambda\\textbf{x}.$$\n",
        "We call $\\textbf{x}$ an _eigenvector_. The span of an eigenvector is called an _eigenspace_.\n",
        "\n",
        "Any square matrix $A \\in \\mathbb{R}^{n \\times n}$ has at most $n$ eigenvalues. Moreover, two eigenvectors $\\textbf{x}_i$ and $\\textbf{x}_j$ corresponding to distinct eigenvalues $\\lambda_i$ and $\\lambda_j$ are linearly independent of each other.\n",
        "\n",
        "If $A$ has $n$ distinct eigenvalues, then we can represent $A$ as\n",
        "$$A = PDP^{-1},$$\n",
        "where $D = \\text{diag}(\\lambda_1, \\dots, \\lambda_n)$ and $P = (\\textbf{x}_1 \\dots \\textbf{x}_n)$. In this case, we say that $A$ is _diagonalizable_.\n",
        "\n",
        "**Theorem**: If $A$ is symmetric, then any two eigenvectors from different eigenspaces are orthogonal.\n",
        "\n",
        "A matrix $A$ is said to be orthogonally diagonalizable if there is an orthogonal matrix $P$ (i.e. $P^{-1} = P^T$) and a diagonal matrix $D$ such that\n",
        "$$A = PDP^T = PDP^{-1}.$$\n",
        "\n",
        "**Theorem** (The spectral theorem for symmatric matrices): An $n \\times n$ symmetric matric $A$ has the following properties:\n",
        "\n",
        "1) $A$ has $n$ real eigenvalues, counting multiplicities.\n",
        "\n",
        "2) If $\\lambda$ is an eigenvalue of $A$ with multiplicity $k$, then the eigespace for $\\lambda$ is $k$-dimensional.\n",
        "\n",
        "3) The eigenspaces are mutually orthogonal, in the sense that eigenvectors corresponding to different eigenvalues are orthogonal.\n",
        "\n",
        "4) $A$ is orthogonally diagonalizable.\n",
        "\n"
      ],
      "metadata": {
        "id": "CPShUX_Skad9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# In the case of a non-symmetric matrix\n",
        "\n",
        "A = np.array([[4.0, -1.0, 0.0], [2.0, 3.0, 5.0], [-1.0, 7.0, 2.0]])\n",
        "\n",
        "lam, P = np.linalg.eig(A)\n",
        "\n",
        "print('The eigenvalues of A are: \\n', lam)\n",
        "print('\\nThe corresponding eigenvectors of A are:\\n ', P)\n",
        "\n",
        "D = np.zeros([3, 3])\n",
        "for i in range(A.ndim+1):\n",
        "  D[i, i] = lam[i]\n",
        "\n",
        "print('\\nComputing A from its diagonalization:\\n', np.matmul(np.matmul(P, D), np.linalg.inv(P)))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ptK7KYWvEBWO",
        "outputId": "86c94bad-4e10-41ea-f8f2-bc3568133390"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The eigenvalues of A are: \n",
            " [-3.25431586  3.96784095  8.28647491]\n",
            "\n",
            "The corresponding eigenvectors of A are:\n",
            "  [[ 0.08351193  0.93004426 -0.1512737 ]\n",
            " [ 0.60582193  0.02990934  0.64843091]\n",
            " [-0.79120512 -0.36622821  0.74609223]]\n",
            "\n",
            "Computing A from its diagonalization:\n",
            " [[ 4.00000000e+00 -1.00000000e+00  1.54651897e-15]\n",
            " [ 2.00000000e+00  3.00000000e+00  5.00000000e+00]\n",
            " [-1.00000000e+00  7.00000000e+00  2.00000000e+00]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# In the case of a symmetric matrix\n",
        "\n",
        "A = np.array([[1, 2, 3], [2, 1, 2], [3, 2, 1]])\n",
        "\n",
        "lam, P = np.linalg.eig(A)\n",
        "\n",
        "print('The eigenvalues of A are: \\n', lam)\n",
        "print('\\nThe corresponding eigenvectors of A are:\\n ', P)\n",
        "\n",
        "D = np.zeros([3, 3])\n",
        "for i in range(A.ndim+1):\n",
        "  D[i, i] = lam[i]\n",
        "\n",
        "print('\\nComputing A from its diagonalization:\\n', np.matmul(np.matmul(P, D), np.linalg.inv(P)))\n",
        "\n",
        "# Eigenvectors from different eigenspaces are orthogonal\n",
        "print('\\nComputing the inner product of x_1 and x_2, corresponding to lambda_1 and lambda_2:\\n', np.dot(P[:, 0], P[:, 1]))\n",
        "print('\\nThe norm of x_1:', np.linalg.norm(P[:, 0]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4G0IAnWVSQ-2",
        "outputId": "86609ae4-81b7-4ab8-ad1c-3bc9b1b40c78"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The eigenvalues of A are: \n",
            " [ 5.70156212 -2.         -0.70156212]\n",
            "\n",
            "The corresponding eigenvectors of A are:\n",
            "  [[-6.05912800e-01 -7.07106781e-01  3.64512933e-01]\n",
            " [-5.15499134e-01  1.47635207e-16 -8.56890100e-01]\n",
            " [-6.05912800e-01  7.07106781e-01  3.64512933e-01]]\n",
            "\n",
            "Computing A from its diagonalization:\n",
            " [[1. 2. 3.]\n",
            " [2. 1. 2.]\n",
            " [3. 2. 1.]]\n",
            "\n",
            "Computing the inner product of x_1 and x_2, corresponding to lambda_1 and lambda_2:\n",
            " -2.220446049250313e-16\n",
            "\n",
            "The norm of x_1: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **_1.2.4.2: Constrained Opimization_**\n",
        "\n",
        "The following is useful for many optimization problems.\n",
        "\n",
        "**Theorem**: Let $A$ be an $n \\times n$ symmetric matrix $A$ with an orthogonal diagonalization $A = PDP^{-1}$. The columns of $P$ are orthonormal eigenvectors $\\textbf{x}_1, \\dots, \\textbf{x}_n$ of $A$. Assume that the diagonals of $D$ are arranged so that $\\lambda_1 \\leq \\lambda_2 \\leq \\dots \\leq \\lambda_n$. Then\n",
        "$$\\text{min}_{\\textbf{x} \\neq 0}\\frac{\\textbf{x}^TA\\textbf{x}}{\\textbf{x}^T\\textbf{x}} = \\lambda_1$$\n",
        "is achieved when $\\textbf{x} = \\textbf{x}_1$ and\n",
        "$$\\text{max}_{\\textbf{x} \\neq 0}\\frac{\\textbf{x}^TA\\textbf{x}}{\\textbf{x}^T\\textbf{x}} = \\lambda_n$$\n",
        "is achieved when $\\textbf{x} = \\textbf{x}_n$."
      ],
      "metadata": {
        "id": "C1Q9MN2XFLGd"
      }
    }
  ]
}